{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'lab_scripts.models.x_autoencoder.data' from '/home/simon/nips/lab_scripts/models/x_autoencoder/data.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lab_scripts.utils import utils\n",
    "utils.change_directory_to_repo()\n",
    "utils.set_deafult_seed()\n",
    "\n",
    "from lab_scripts.models.x_autoencoder import model, data\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "importlib.reload(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'data': 'mp/official/gex_to_adt',\n",
    "    'normalize': True,\n",
    "    'top_n_genes': 2000,\n",
    "\n",
    "    'first_dims': [2000, 700, ],\n",
    "    'second_dims': [134, 200, ],\n",
    "    'latent_dim': 500,\n",
    "    'activation': 'tanh',\n",
    "    'lr': 0.01,\n",
    "    'batch_size': 128,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:x_autoencoder:First encoder dims: [2000, 700, 500, 500]\n",
      "INFO:x_autoencoder:Second encoder dims: [134, 350, 400, 500]\n"
     ]
    }
   ],
   "source": [
    "model = model.X_autoencoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.GexAdtData(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer(gpus=1, overfit_batches=1, max_epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type    | Params\n",
      "-------------------------------------------\n",
      "0 | loss           | MSELoss | 0     \n",
      "1 | first_encoder  | Encoder | 2.0 M \n",
      "2 | first_decoder  | Decoder | 2.0 M \n",
      "3 | second_encoder | Encoder | 388 K \n",
      "4 | second_decoder | Decoder | 387 K \n",
      "-------------------------------------------\n",
      "4.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.8 M     Total params\n",
      "19.123    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/miniconda3/envs/nips/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/simon/miniconda3/envs/nips/lib/python3.8/site-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "Global seed set to 228\n",
      "/home/simon/miniconda3/envs/nips/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/simon/miniconda3/envs/nips/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192: 100%|██████████| 2/2 [00:00<00:00, 95.11it/s, loss=0.0819, v_num=33, train_loss_step=0.0819, val_loss=0.0819, train_loss_epoch=0.0819] Epoch   193: reducing learning rate of group 0 to 2.0000e-03.\n",
      "Epoch 286: 100%|██████████| 2/2 [00:00<00:00, 90.17it/s, loss=0.0818, v_num=33, train_loss_step=0.0818, val_loss=0.0818, train_loss_epoch=0.0818]Epoch   287: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 315: 100%|██████████| 2/2 [00:00<00:00, 93.46it/s, loss=0.0818, v_num=33, train_loss_step=0.0818, val_loss=0.0818, train_loss_epoch=0.0818] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/miniconda3/envs/nips/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1046: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss_step': tensor(0.0156),\n",
       " 'first_to_first_step': tensor(0.0054),\n",
       " 'first_to_second_step': tensor(0.0027),\n",
       " 'second_to_first_step': tensor(0.0053),\n",
       " 'second_to_second_step': tensor(0.0022),\n",
       " 'val_loss': 0.015600309707224369,\n",
       " 'epoch': 4397,\n",
       " 'train_loss_epoch': 0.01560253743082285,\n",
       " 'first_to_first_epoch': 0.005443661939352751,\n",
       " 'first_to_second_epoch': 0.002684005768969655,\n",
       " 'second_to_first_epoch': 0.00529222609475255,\n",
       " 'second_to_second_epoch': 0.0021826433949172497}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.logged_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.logged_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'train_loss_step': tensor(0.0219),\n",
    " 'first_to_first_step': tensor(0.0077),\n",
    " 'first_to_second_step': tensor(0.0043),\n",
    " 'second_to_first_step': tensor(0.0077),\n",
    " 'second_to_second_step': tensor(0.0022),\n",
    " 'val_loss': 0.02187570556998253,\n",
    " 'epoch': 1105,\n",
    " 'train_loss_epoch': 0.02187589555978775,\n",
    " 'first_to_first_epoch': 0.007687397301197052,\n",
    " 'first_to_second_epoch': 0.004332174081355333,\n",
    " 'second_to_first_epoch': 0.007693755906075239,\n",
    " 'second_to_second_epoch': 0.002162568038329482}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "56a5fcc7b7e2612f6fc27864a94affd768eb55fcd75f3241521b5f5947107b33"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('nips': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
